{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "# import tf.keras.backend as K\n",
        "from keras.activations import elu\n",
        "import cv2, itertools, sys, editdistance, math\n",
        "from tensorflow.keras.backend import ctc_batch_cost as ctcLoss\n",
        "\n",
        "seed = 13\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n"
      ],
      "metadata": {
        "id": "ggW8QS4ygs2f"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data_utils.py"
      ],
      "metadata": {
        "id": "oiBnOurYgwrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncateLabel(text, maxStringLen = 32):\n",
        "\tcost = 0\n",
        "\tfor i in range(len(text)):\n",
        "\t\tif i!=0 and text[i] == text[i-1]:\n",
        "\t\t\tcost+=2\n",
        "\t\telse:\n",
        "\t\t\tcost+=1\n",
        "\t\tif cost > maxStringLen:\n",
        "\t\t\treturn text[:i]\n",
        "\treturn text\n",
        "\n",
        "def textToLabels(text, unicodes):\n",
        "\tret = []\n",
        "\tfor c in text:\n",
        "\t\tret.append(unicodes.index(c))\n",
        "\treturn ret\n",
        "\n",
        "def labelsToText(labels, unicodes):\n",
        "\tret = []\n",
        "\tfor c in labels:\n",
        "\t\tif c == len(unicodes):\n",
        "\t\t\tret.append(\"\")\n",
        "\t\telse:\n",
        "\t\t\tret.append(unicodes[c])\n",
        "\treturn \"\".join(ret)\n",
        "\n",
        "def preprocess(img, dataAugmentation = False):\n",
        "\t(wt, ht) = (128, 32)\n",
        "\tif img is None:\n",
        "\t\timg = (np.zeros((wt, ht, 1))).astype('uint8')\n",
        "\timg = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] * 255\n",
        "\n",
        "\tif dataAugmentation:\n",
        "\t\tstretch = (random.random() - 0.5) \t\t\t\t\t\t# -0.5 .. +0.5\n",
        "\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1)  # random width, but at least 1\n",
        "\t\timg = cv2.resize(img, (wStretched, img.shape[0])) \t\t# stretch horizontally by factor 0.5 .. 1.5\n",
        "\timg = closeFit(img)                                         # to avoid lot of white space around text\n",
        "\n",
        "\th = img.shape[0]\n",
        "\tw = img.shape[1]\n",
        "\tfx = w / wt\n",
        "\tfy = h / ht\n",
        "\tf = max(fx, fy)\n",
        "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) \t#scale according to f (result at least 1 and at most wt or ht)\n",
        "\timg = cv2.resize(img, newSize, interpolation = cv2.INTER_AREA)   \t\t#INTER_AREA important, Linear loses all info\n",
        "\timg = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] * 255\n",
        "\n",
        "\ttarget = np.ones([ht, wt]) * 255\n",
        "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
        "\timg = cv2.transpose(target)\n",
        "\t(m, s) = cv2.meanStdDev(img)\n",
        "\tm = m[0][0]\n",
        "\ts = s[0][0]\n",
        "\timg = img - m\n",
        "\timg = img / s if s>1e-3 else img\n",
        "\treturn np.reshape(img, (img.shape[0], img.shape[1], 1))\n",
        "\n",
        "def closeFit(img):\n",
        "\ti = 2\n",
        "\tcol = 255 - np.sum(img, axis=0)/img.shape[0]\n",
        "\twhile i<img.shape[1] and col[i]<=5:\n",
        "\t\ti+=1\n",
        "\tw1 = max(0,i - 15)\n",
        "\ti = img.shape[1]-1\n",
        "\twhile i>=0 and col[i]<=5:\n",
        "\t\ti-=1\n",
        "\tw2 = i + 15\n",
        "\n",
        "\trow = 255 - np.sum(img, axis=1)/img.shape[1]\n",
        "\ti = 2\n",
        "\twhile i<img.shape[0] and row[i]<=4:\n",
        "\t\ti+=1\n",
        "\th1 = max(0,i - 20)\n",
        "\ti = img.shape[0] - 1\n",
        "\twhile i>=0 and row[i]<=5:\n",
        "\t\ti-=1\n",
        "\th2 = i + 20\n",
        "\tfinal = img[h1:h2,w1:w2]\n",
        "\tif final.shape[0]*final.shape[1] == 0:\n",
        "\t\treturn img\n",
        "\treturn final"
      ],
      "metadata": {
        "id": "WSdsXzLWgzN3"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model_utils.py"
      ],
      "metadata": {
        "id": "DUgel75bg1Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictImage(imgPath, weightPath):\n",
        "\timg = cv2.imread(imgPath, 0)\n",
        "\timg = preprocess(img, False)\n",
        "\timg = np.reshape(img, (1, img2.shape[0], img2.shape[1], 1))\n",
        "\tunicodes = list(np.load('unicodes.npy', allow_pickle = True))\n",
        "\tmodel = CRNN(False, len(unicodes + 1))\n",
        "\tmodel.load_weights(weightPath)\n",
        "\tout = model.predict(img2)\n",
        "\tpred = decode(out)\n",
        "\tprint('Recognized Word: '+ str(pred))\n",
        "\n",
        "def ctcLambdaFunc(yPred, labels, inputLength, labelLength):\n",
        "\tyPred = yPred[:,2:,:]\n",
        "\tloss = ctcLoss(labels, yPred, inputLength, labelLength)\n",
        "\treturn loss\n",
        "\n",
        "def decode(yPred, unicodes):  #Best Path Decoder\n",
        "\ttexts = []\n",
        "\tfor y in yPred:\n",
        "\t\tlabel = list(np.argmax(y[2:],1))\n",
        "\t\tlabel = [k for k, g in itertools.groupby(label)]\n",
        "\t\ttext = labelsToText(label, unicodes)\n",
        "\t\ttexts.append(text)\n",
        "\treturn texts\n",
        "\n",
        "def test(model, loader):\n",
        "\tvalidation = loader.valSet\n",
        "\ttrueText = []\n",
        "\tfor (i, path) in validation:\n",
        "\t\ttrueText.append(i)\n",
        "\n",
        "\t# Wrap the output of loader.nextVal in a tuple to match Keras input expectations\n",
        "\n",
        "\tbatch_size = 3  # Set this to your desired batch size\n",
        "\n",
        "\t# Define output_signature for the validation data generator\n",
        "\toutput_signature = (\n",
        "\t\ttf.TensorSpec(shape=(batch_size, 128, 32, 1), dtype=tf.float32),\n",
        "\t)\n",
        "\n",
        "\tvalidation_data = tf.data.Dataset.from_generator(\n",
        "\t\tlambda: loader.nextVal(batch_size),\n",
        "\t\toutput_signature=output_signature\n",
        "\t)\n",
        "\n",
        "\toutputs = model.predict(validation_data, steps=math.ceil(loader.valLength / batch_size))\n",
        "\tunicodes = list(np.load('/content/unicodes.npy', allow_pickle = True))\n",
        "\tpredText = decode(outputs, unicodes)\n",
        "\n",
        "\tprint(predText)\n",
        "\n",
        "\twordOK = 0\n",
        "\twordTot = 0\n",
        "\tcharDist = 0\n",
        "\tcharTot = 0\n",
        "\tfor i in range(len(trueText)):\n",
        "\t\t#print(predText[i], trueText[i])\n",
        "\t\twordOK += 1 if predText[i] == trueText[i] else 0\n",
        "\t\twordTot += 1\n",
        "\t\tdist = editdistance.eval(predText[i], trueText[i])\n",
        "\t\tcharDist += dist\n",
        "\t\tcharTot += len(trueText[i])\n",
        "\n",
        "\tCAR = 100 - 100 * charDist/charTot\n",
        "\tWAR = 100 * wordOK/wordTot\n",
        "\tprint('Character Accuracy Rate (CAR):' + str(CAR))\n",
        "\tprint('Word Accuracy Rate (WAR):' + str(WAR))\n",
        "\treturn (CAR, WAR)"
      ],
      "metadata": {
        "id": "bsounZSGg4zc"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRNN.py"
      ],
      "metadata": {
        "id": "plauvzySg_yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tensor = tf.constant([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]])\n",
        "\n",
        "# Reverse along axis 1 (columns)\n",
        "tf.keras.backend.reverse(tensor, axes=[0])"
      ],
      "metadata": {
        "id": "l1_WKQqUWBFI",
        "outputId": "17d5671b-ba30-4f40-b2a7-ff9d4c681d12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
              "array([[2, 3, 4, 5, 6],\n",
              "       [1, 2, 3, 4, 5]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CRNN(train, outClasses):\n",
        "\tinputShape = (128, 32, 1)\n",
        "\tkernels = [5, 5, 3, 3, 3]\n",
        "\tfilters = [32, 64, 128, 128, 256]\n",
        "\tstrides = [(2,2), (2,2), (1,2), (1,2), (1,2)]\n",
        "\trnnUnits = 256\n",
        "\tmaxStringLen = 32\n",
        "\n",
        "\tinputs = Input(name = 'inputX', shape = inputShape, dtype = 'float32')\n",
        "\tlabels = Input(name='label', shape=[maxStringLen], dtype='float32')\n",
        "\tinputLength = Input(name='inputLen', shape=[1], dtype='int64')\n",
        "\tlabelLength = Input(name='labelLen', shape=[1], dtype='int64')\n",
        "\n",
        "\tinner = inputs\n",
        "\tfor i in range(len(kernels)):\n",
        "\t\tinner = Conv2D(filters[i], (kernels[i], kernels[i]), padding = 'same',\\\n",
        "\t\t\t\t\t   name = 'conv' + str(i+1), kernel_initializer = 'glorot_normal') (inner)\n",
        "\t\tinner = BatchNormalization() (inner)\n",
        "\t\tinner = Activation(elu) (inner)\n",
        "\t\tinner = MaxPooling2D(pool_size = strides[i], name = 'max' + str(i+1)) (inner)\n",
        "\tinner = Reshape(target_shape = (maxStringLen,rnnUnits), name = 'reshape')(inner)\n",
        "\n",
        "\tLSF = LSTM(rnnUnits, return_sequences=True, kernel_initializer='glorot_normal', name='LSTM1F') (inner)\n",
        "\tLSB = LSTM(rnnUnits, return_sequences=True, go_backwards = True, kernel_initializer='glorot_normal', name='LSTM1B') (inner)\n",
        "\tLSB = Lambda(lambda inputTensor: tf.keras.backend.reverse(inputTensor, axes=[1]), output_shape=(maxStringLen, rnnUnits)) (LSB)\n",
        "\tLS1 = Average()([LSF, LSB])\n",
        "\tLS1 = BatchNormalization() (LS1)\n",
        "\n",
        "\tLSF = LSTM(rnnUnits, return_sequences=True, kernel_initializer='glorot_normal', name='LSTM2F') (LS1)\n",
        "\tLSB = LSTM(rnnUnits, return_sequences=True, go_backwards = True, kernel_initializer='glorot_normal', name='LSTM2B') (LS1)\n",
        "\tLSB = Lambda(lambda inputTensor: tf.keras.backend.reverse(inputTensor, axes=[1]), output_shape=(maxStringLen, rnnUnits)) (LSB)\n",
        "\tLS2 = Concatenate()([LSF, LSB])\n",
        "\tLS2 = BatchNormalization() (LS2)\n",
        "\n",
        "\tyPred = Dense(outClasses, kernel_initializer='glorot_normal', name='dense2') (LS2)\n",
        "\tyPred = Activation('softmax', name='softmax') (yPred)\n",
        "\tlossOut = Lambda(ctcLambdaFunc, output_shape=(1,), name='ctc') ([yPred, labels, inputLength, labelLength])\n",
        "\n",
        "\t# if train:\n",
        "\t# \treturn Model(inputs=[inputs, labels, inputLength, labelLength], outputs=[lossOut, yPred])\n",
        "\treturn Model(inputs=[inputs], outputs=yPred)"
      ],
      "metadata": {
        "id": "Gyxg5EoChCB0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader.py"
      ],
      "metadata": {
        "id": "EEHb529ZhECW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader():\n",
        "\tdef __init__(self, trainFile, valFile, unicodes):\n",
        "\t\tself.unicodes = unicodes\n",
        "\t\tself.trainFile = trainFile\n",
        "\t\tself.valFile = valFile\n",
        "\t\tself.maxStringLen = 32\n",
        "\t\tself.trainSet = []\n",
        "\t\tself.valSet = []\n",
        "\t\tself.trainIndex = 0\n",
        "\t\tself.valIndex = 0\n",
        "\n",
        "\t\tif self.trainFile != \"\":\n",
        "\t\t\tself.trainSet = self.importSets(True)\n",
        "\t\tif self.valFile != \"\":\n",
        "\t\t\tself.valSet = self.importSets(False)\n",
        "\t\tself.valLength = len(self.valSet)\n",
        "\t\tself.trainLength = len(self.trainSet)\n",
        "\n",
        "\tdef importSets(self, train):\n",
        "\t\tset = []\n",
        "\t\tif train:\n",
        "\t\t\tfile = open(self.trainFile, 'r', encoding='utf-8')\n",
        "\t\telse:\n",
        "\t\t\tfile = open(self.valFile, 'r', encoding='utf-8')\n",
        "\t\tfor line in file:\n",
        "\t\t\tinUnicodes = True\n",
        "\t\t\tif not line or line[0] =='#':\n",
        "\t\t\t\t#Ignoring Erroneous Lines manually skipped with # in file\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tlineSplit = line.strip().split(' ')\n",
        "\t\t\tif len(lineSplit) >= 2:\n",
        "\t\t\t\tfileName = lineSplit[0]\n",
        "\t\t\t\ttext = truncateLabel(' '.join(lineSplit[1:]))\n",
        "\n",
        "\t\t\t\tfor ch in text:\n",
        "\t\t\t\t\tif not ch in self.unicodes:\n",
        "\t\t\t\t\t\tprint('Char '+ str(ch)+ ' Not in Unicodes, and Word Omitted')\n",
        "\t\t\t\t\t\t#print(ch,('0'+hex(ord(ch))[2:]))\n",
        "\t\t\t\t\t\tinUnicodes = False\n",
        "\n",
        "\t\t\t\tif inUnicodes:\n",
        "\t\t\t\t\tif train:\n",
        "\t\t\t\t\t\tset.append((text, fileName))\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tset.append((text, fileName))\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(line + 'Check this Line')\n",
        "\t\tfile.close()\n",
        "\t\trandom.shuffle(set)\n",
        "\t\treturn set\n",
        "\n",
        "\tdef nextTrain(self, batchSize):\n",
        "\t\twhile True:\n",
        "\t\t\tif self.trainIndex + batchSize >= self.trainLength:\n",
        "\t\t\t\tself.trainIndex = 0\n",
        "\t\t\t\trandom.shuffle(self.trainSet)\n",
        "\t\t\tret = self.getBatch(self.trainIndex, batchSize, True)\n",
        "\t\t\tself.trainIndex += batchSize\n",
        "\t\t\tyield ret\n",
        "\n",
        "\tdef nextVal(self, batchSize):\n",
        "\t\twhile True:\n",
        "\t\t\tif self.valIndex >= self.valLength:\n",
        "\t\t\t\tself.valIndex = 0\n",
        "\t\t\tret = self.getBatch(self.valIndex, batchSize, False)\n",
        "\t\t\tself.valIndex += batchSize\n",
        "\t\t\tyield (ret,)\n",
        "\n",
        "\tdef getBatch(self, index, batchSize, train):\n",
        "\t\tif train:\n",
        "\t\t\tbatch = self.trainSet[index:index + batchSize]\n",
        "\t\t\tsize = self.trainLength\n",
        "\t\telse:\n",
        "\t\t\tbatch = self.valSet[index:index + batchSize]\n",
        "\t\t\tsize = self.valLength\n",
        "\n",
        "\t\timgs = []\n",
        "\t\tlabels = np.ones([batchSize, self.maxStringLen]) * len(self.unicodes)\n",
        "\t\tinputLength = np.zeros([batchSize, 1])\n",
        "\t\tlabelLength = np.zeros([batchSize, 1])\n",
        "\n",
        "\t\tfor i in range(min(batchSize, size-index)):\n",
        "\t\t\timg = cv2.imread(batch[i][1], 0)\n",
        "\t\t\tif img is None:\n",
        "\t\t\t\timg = np.zeros((128,32,1))\n",
        "\t\t\t\tprint(batch[i][1] + 'is not available')\n",
        "\n",
        "\t\t\timg = img.astype('uint8')\n",
        "\t\t\timgs.append(preprocess(img.astype('uint8'), train))\n",
        "\t\t\tlabels[i, 0:len(batch[i][0])] = textToLabels(batch[i][0], self.unicodes)\n",
        "\t\t\tlabelLength[i] = len(batch[i][0])\n",
        "\t\t\tinputLength[i] = self.maxStringLen - 2\n",
        "\n",
        "\t\tinputs = {\n",
        "\t\t\t\t'inputX' : np.asarray(imgs),\n",
        "\t\t\t\t'label' : labels,\n",
        "\t\t\t\t'inputLen' : inputLength,\n",
        "\t\t\t\t'labelLen' : labelLength,\n",
        "\t\t\t\t\t}\n",
        "\t\toutputs = {'ctc' : np.zeros([batchSize])}\n",
        "\t\tif train:\n",
        "\t\t\treturn (inputs, outputs)\n",
        "\t\telse:\n",
        "\t\t\treturn imgs"
      ],
      "metadata": {
        "id": "g2_5B3p9hGSQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainer.py"
      ],
      "metadata": {
        "id": "XoXEJMMDhIHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unicodes = list(np.load('/content/unicodes.npy', allow_pickle = True))\n",
        "\n",
        "loader = DataLoader('', \"/content/test.txt\", unicodes)\n",
        "testModel = CRNN(False, len(unicodes) + 1)\n",
        "testModel.load_weights('/content/crnn_weights_exp2.h5')\n",
        "CAR, WAR = test(testModel, loader)"
      ],
      "metadata": {
        "id": "Fdj0jU4khNtr",
        "outputId": "2596a9e0-c594-4f56-a0a5-575a4a09973e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672ms/step\n",
            "['मनमैला', 'लसिका', 'अलबकाइएगी']\n",
            "Character Accuracy Rate (CAR):95.0\n",
            "Word Accuracy Rate (WAR):66.66666666666667\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}